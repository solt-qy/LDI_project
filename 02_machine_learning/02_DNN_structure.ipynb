{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a2a9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f49ebb",
   "metadata": {},
   "source": [
    "# DNN model 1 (Traditional CNN)\n",
    "\n",
    "A reasonably standard CNN structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "447d6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, BatchNormalization, GlobalAvgPool2D, AvgPool2D, MaxPool2D, LeakyReLU, Concatenate, Dropout, SpatialDropout2D\n",
    "\n",
    "def simple_cnn_model(input_shape, num_classes):\n",
    "    filters = [32, 64, 128]\n",
    "    bn_momentum=0.99\n",
    "    leaky_alpha = 0.05\n",
    "    dropout = 0.1\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # first convolutional block\n",
    "    # small filters\n",
    "    features = BatchNormalization(momentum=bn_momentum)(inputs)\n",
    "    features = LeakyReLU(leaky_alpha)(features)\n",
    "    features = Conv2D(filters[0], (5,5), padding='same')(features)\n",
    "    features = Concatenate()([AvgPool2D(pool_size=(2,2))(features),MaxPool2D(pool_size=(2,2))(features)])\n",
    "    features = SpatialDropout2D(dropout)(features)\n",
    "    \n",
    "    # second convolutional block\n",
    "    # moderate filters\n",
    "    features = BatchNormalization(momentum=bn_momentum)(features)\n",
    "    features = LeakyReLU(leaky_alpha)(features)\n",
    "    features = Conv2D(filters[1], (5,5), padding='same')(features)\n",
    "    features = Concatenate()([AvgPool2D(pool_size=(2,2))(features),MaxPool2D(pool_size=(2,2))(features)])\n",
    "    features = SpatialDropout2D(dropout)(features)\n",
    "    \n",
    "    # third convolutional block\n",
    "    # moderate filters\n",
    "    features = BatchNormalization(momentum=bn_momentum)(features)\n",
    "    features = LeakyReLU(leaky_alpha)(features)\n",
    "    features = Conv2D(filters[2], (3,3), padding='same')(features)\n",
    "    features = GlobalAvgPool2D()(features)\n",
    "    features = Dropout(dropout)(features)\n",
    "\n",
    "    # Dense classification\n",
    "    classification = Dense(32)(features)\n",
    "    classification = LeakyReLU(leaky_alpha)(classification)\n",
    "    classification = Dense(num_classes, activation='sigmoid')(classification)\n",
    "\n",
    "    model = Model(inputs, classification, name='cnn_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ef2b6",
   "metadata": {},
   "source": [
    "# DNN model 2 (Densenet)\n",
    "\n",
    "Below is our own implementation of densenet, following the tutorial in\n",
    "https://amaarora.github.io/2020/08/02/densenets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7afe82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dense_net(initial_feature, num_label, input_shape, \n",
    "              dense_block_config, drop_out = 0.2, bottle_necksz=4, growth_rate=32):\n",
    "    \n",
    "    def dense_block(input_layer, num_sets, bottle_necksz, growth_rate):\n",
    "        layer_sets = [input_layer]\n",
    "        for i in range(num_sets):\n",
    "            if i > 0:\n",
    "                input_layer = keras.layers.Concatenate()(layer_sets)\n",
    "                layer_sets = []\n",
    "                layer_sets.append(input_layer)\n",
    "            bottleneck_1 = keras.layers.BatchNormalization()(input_layer)\n",
    "            activation_1 = keras.layers.ReLU()(bottleneck_1)\n",
    "            convolution_1 = keras.layers.Conv2D(bottle_necksz*growth_rate,\n",
    "                                                kernel_size=(1,1), strides=1, use_bias=False)(activation_1)\n",
    "            bottleneck_2 = keras.layers.BatchNormalization()(convolution_1)\n",
    "            activation_2 =  keras.layers.ReLU()(bottleneck_2)\n",
    "            convolution_2 = keras.layers.Conv2D(growth_rate, kernel_size=(3,3), \n",
    "                                                strides=1, padding='same', use_bias=False)(activation_2)\n",
    "            layer_sets.append(convolution_2)\n",
    "        return keras.layers.Concatenate()(layer_sets)\n",
    "\n",
    "    def transition_layer(input_layer):\n",
    "        batch_norm = keras.layers.BatchNormalization()(input_layer)\n",
    "        activation = keras.layers.ReLU()(batch_norm)\n",
    "        feature_size = keras.backend.int_shape(activation)[3]\n",
    "        conv = keras.layers.Conv2D(feature_size//2, kernel_size=(1,1),strides=1,use_bias=False)(activation)\n",
    "        pool = keras.layers.AveragePooling2D()(conv)\n",
    "        return pool\n",
    "\n",
    "    def fully_connected_layer(input_layer, num_labels):\n",
    "        pool = keras.layers.GlobalAveragePooling2D()(input_layer)\n",
    "        norm_1 = keras.layers.BatchNormalization()(pool)\n",
    "        dropout = keras.layers.Dropout(.2)(norm_1)\n",
    "        dense_1 = keras.layers.Dense(1024, activation='relu')(dropout)\n",
    "        dense_2 = keras.layers.Dense(512, activation='relu')(dense_1)\n",
    "        norm_2 = keras.layers.BatchNormalization()(dense_2)\n",
    "        dropout_2 = keras.layers.Dropout(.2)(norm_2)\n",
    "        return keras.layers.Dense(num_labels, activation='softmax')(dropout_2)\n",
    "\n",
    "    inputs = keras.Input(shape = input_shape)\n",
    "    # initial transition layers\n",
    "    initial_padding_1 = keras.layers.ZeroPadding2D(padding=(3,3))(inputs)\n",
    "    initial_conv = keras.layers.Conv2D(initial_feature, kernel_size=(7,7), \n",
    "                                       strides=2, use_bias=False)(initial_padding_1)\n",
    "    initial_norm = keras.layers.BatchNormalization()(initial_conv)\n",
    "    initial_relu = keras.layers.ReLU()(initial_norm)\n",
    "    initial_padding_2 = keras.layers.ZeroPadding2D(padding=(1,1))(initial_relu)\n",
    "    initial = keras.layers.MaxPooling2D(pool_size=(3,3), strides=2)(initial_padding_2)\n",
    "    \n",
    "    for num in dense_block_config:\n",
    "        conv = dense_block(initial, num, bottle_necksz, growth_rate)\n",
    "        initial = transition_layer(conv)\n",
    "\n",
    "    outputs = fully_connected_layer(initial, num_label)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03012c7",
   "metadata": {},
   "source": [
    "## DNN model 2 (Convolutional LSTM)\n",
    "Below is the Convolutional LSTM used in https://github.com/WWH98932/Audio-Classification-Models\n",
    "\n",
    "ResNet50 is discussed in the project report. The original paper for resnet is at https://arxiv.org/pdf/1512.03385.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9143e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras import regularizers\n",
    "\n",
    "def resnet_ldnn(num_label):\n",
    "    model = Sequential()\n",
    "    model.add(keras.applications.resnet50.ResNet50(include_top=False, input_shape=(128, 126, 1), \n",
    "                                                   weights=None, classes=None, pooling='average'))\n",
    "    model.add(Permute((2, 1, 3)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(64, dropout=0.25, return_sequences=True))\n",
    "    model.add(LSTM(64, dropout=0.25))\n",
    "    model.add(Dense(64))\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_label, kernel_regularizer=regularizers.l2(0.01), activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadadf29",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1278e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import ast\n",
    "\n",
    "# Read data from path\n",
    "# dataframe is ideally the pickled output from 01_dataset_curation.select_training_data()\n",
    "data_root = pathlib.Path('/path/to/training/dataframe/')\n",
    "training_df = pd.read_pickle(data_root/'01_manifest.pkl')\n",
    "training_df.head()\n",
    "\n",
    "len(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fc503a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['animal_birds', 'animal_dogs', 'background', 'human_voice',\n",
       "       'mechanical', 'transport_car'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "considered_categories_large = [\"animal_dogs\", \"animal_insects\", \"animal_birds\", \"animal_cockatoo\", \"animal_poultry\",  \"background\", \"human_voice\", \"indeterminate\", \"mechanical\", \"mechanical_construction\", \"mechanical_impulsive\", \"mechanical_plant\", \"nature_wind\", \"signals_horn\", \"signals_siren\", \"transport_car\", 'music']\n",
    "considered_categories_small = ['animal_dogs', 'animal_birds', 'human_voice', 'transport_car', 'mechanical', 'music']\n",
    "binary_dogs = ['animal_dogs']\n",
    "# change this line to the categorisation you need\n",
    "considered_categories = considered_categories_small\n",
    "class OneVsOtherBinarizer(object):\n",
    "    # simple dummy class for a fit for purpose one vs others binariser\n",
    "    # keep a similar api to other binarisers used to avoid modifying code down the track\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "    def fit(self, the_one):\n",
    "        self.the_one = the_one\n",
    "        self.the_others = 'not_'+the_one\n",
    "        self.classes_ = np.array([self.the_others, the_one])\n",
    "        return self\n",
    "    def transform(self, data):\n",
    "        _bin = np.array([self.the_one in d for d in data])\n",
    "        _bin = _bin.astype(np.int32)\n",
    "        return _bin\n",
    "\n",
    "\n",
    "def get_category_encoder(categories):\n",
    "    # return an appropriate encoder for the classification problem\n",
    "    # again, quite fit for purpose\n",
    "    if len(categories) == 1:\n",
    "        return OneVsOtherBinarizer().fit(categories[0])\n",
    "    else:\n",
    "        return MultiLabelBinarizer().fit([categories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c40ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These spectrogram settings look pretty good from a domain perspective.\n",
    "# Trying a little bit higher \"resolution\" than previously\n",
    "mel_settings = {'fmax': 8000, 'power': 2, 'n_mels' :128, 'n_fft':2048, 'hop_length':512}\n",
    "fs_nom = 16000 # Nominal sampling rate. Most files should be this rate, but if not, they will be resampled\n",
    "shape_nom = (128,126) # nominal spectrogram shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c4ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import sklearn\n",
    "\n",
    "def force_array_shape(x, force_shape):\n",
    "    \"\"\"Forces a numpy array to a specific shape by filling with zeros, or truncating\"\"\"\n",
    "    pad_widths = []\n",
    "    for ax, ax_length in enumerate(force_shape):\n",
    "        if x.shape[ax] >= ax_length:\n",
    "            x = x.take(indices=range(0,ax_length), axis=ax)\n",
    "        pad_widths.append((0,ax_length-x.shape[ax]))\n",
    "    x = np.pad(x, pad_widths)\n",
    "    return x\n",
    "\n",
    "def get_mels(filepath='', data=[], fs=None, force_shape=None):\n",
    "    if filepath:\n",
    "        data, fs = librosa.load(filepath, sr=fs)\n",
    "        if fs != fs_nom:\n",
    "            print(filepath)\n",
    "    else:\n",
    "        assert (len(data>0) and fs >0), 'Must provide either a filename, or array of data and sample rate'\n",
    "    \n",
    "    S = librosa.feature.melspectrogram(y=data, sr = fs, **mel_settings)\n",
    "    \n",
    "    if force_shape and S.shape != force_shape:\n",
    "        \n",
    "        S = force_array_shape(S, force_shape)\n",
    "            \n",
    "    return S, fs\n",
    " \n",
    "def load_mels(filepath, force_create=False, save=True):\n",
    "    mel_path = filepath.with_suffix('.npy')\n",
    "    \n",
    "    if mel_path.is_file() and not force_create:\n",
    "        #print('Loading {}'.format(mel_path))\n",
    "        mels = np.load(mel_path)\n",
    "    else:\n",
    "        #print('Generating from {}'.format(filepath))\n",
    "        mels, _ = get_mels(filepath, fs=fs_nom, force_shape = shape_nom)\n",
    "        if save:\n",
    "            #print('Saving {}'.format(mel_path))\n",
    "            np.save(mel_path, mels)\n",
    "    \n",
    "    return mels\n",
    "\n",
    "def feature_preprocessing(mel):\n",
    "    # convert to db and normalise\n",
    "    power = librosa.core.power_to_db(mel, ref=np.max)\n",
    "    power = power - np.mean(power)\n",
    "    power = power / (np.std(power))\n",
    "    return power[:, :, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ff557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the features\n",
    "# note this will store all features in memory, as well as saving them to disk. \n",
    "# Can't guarantee it will work for large datasets.\n",
    "training_df['features'] = training_df.apply(lambda x: data_root/x['package_hash']/x['filename'], axis=1).apply(lambda x: feature_preprocessing(load_mels(x, force_create=False, save=True)))\n",
    "\n",
    "# sometimes nan's leak in, from bad source data remove them\n",
    "training_df = training_df[~training_df['features'].apply(lambda x: np.any(np.isnan(x.flatten())))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4331b938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ldi/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:875: UserWarning: unknown class(es) ['animal_cockatoo', 'animal_insects', 'animal_other', 'animal_poultry', 'human_movement', 'indeterminate', 'mechanical_construction', 'mechanical_impulsive', 'music', 'nature_wind', 'signals_bell', 'signals_horn', 'signals_siren', 'transport_aircraft', 'transport_motorcycle'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Support\n",
      "animal_birds                  [[1 0 0 0 0 0]] : 2382\n",
      "animal_dogs                   [[0 1 0 0 0 0]] : 2229\n",
      "background                    [[0 0 1 0 0 0]] : 915\n",
      "human_voice                   [[0 0 0 1 0 0]] : 964\n",
      "mechanical                    [[0 0 0 0 1 0]] : 2099\n",
      "transport_car                 [[0 0 0 0 0 1]] : 1251\n",
      "(4379, 128, 126, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.stack(training_df['features'].values)\n",
    "category_encoder = get_category_encoder(considered_categories)\n",
    "y = category_encoder.transform(training_df['category_set'].values)\n",
    "\n",
    "print('Category Support')\n",
    "for c,n in zip(category_encoder.classes_, y.sum(axis=0)):\n",
    "    print('{:30s}{} : {}'.format(c, category_encoder.transform([[c]]), n) )\n",
    "\n",
    "idx_list= list(range(y.shape[0]))\n",
    "for i in range(y.shape[0]):\n",
    "    if np.all((y[i] == 0)):\n",
    "        idx_list.remove(i) \n",
    "X = X[idx_list]\n",
    "y = y[idx_list]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e93434f",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "Run one of these cells to set the model architecture to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28452c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# channel of the first convolutional layer\n",
    "initial_feature = 64  \n",
    "# number of labels to be categorized\n",
    "num_labels = 6\n",
    "input_shape = (128, 126, 1)\n",
    "dense_block_config=(6, 12, 24, 16)\n",
    "model = dense_net(initial_feature, num_labels, input_shape, dense_block_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d26ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet_ldnn(len(category_encoder.classes_))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fc2de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_cnn_model((128, 126, 1),len(category_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce672a9e",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b979b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_count(y_true, y_pred):\n",
    "    # metric to compute the exact match ratio\n",
    "    # this portion counts 0/1 whether prediction exactly matches target.\n",
    "    # use ec.numpy().mean() to get the ratio\n",
    "    predictions = tf.cast(tf.greater_equal(y_pred, 0.5), tf.float32)\n",
    "    pred_match = tf.equal(predictions, tf.round(y_true))\n",
    "    exact_count = tf.math.reduce_min(tf.cast(pred_match, tf.float32), axis=1)\n",
    "    return exact_count\n",
    "\n",
    "def macro_double_soft_f1(y, y_hat):\n",
    "    \"\"\"\n",
    "    Taken Directly from https://towardsdatascience.com/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d\n",
    "    All credit to Ashref Maiza\n",
    "    \n",
    "    Compute the macro soft F1-score as a cost (average 1 - soft-F1 across all labels).\n",
    "    Use probability values instead of binary predictions.\n",
    "    This version uses the computation of soft-F1 for both positive and negative class for each label.\n",
    "    \n",
    "    Args:\n",
    "        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n",
    "        y_hat (float32 Tensor): probability matrix from forward propagation of shape (BATCH_SIZE, N_LABELS)\n",
    "        \n",
    "    Returns:\n",
    "        cost (scalar Tensor): value of the cost function for the batch\n",
    "    \"\"\"\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    tn = tf.reduce_sum((1 - y_hat) * (1 - y), axis=0)\n",
    "    soft_f1_class1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    soft_f1_class0 = 2*tn / (2*tn + fn + fp + 1e-16)\n",
    "    cost_class1 = 1 - soft_f1_class1 # reduce 1 - soft-f1_class1 in order to increase soft-f1 on class 1\n",
    "    cost_class0 = 1 - soft_f1_class0 # reduce 1 - soft-f1_class0 in order to increase soft-f1 on class 0\n",
    "    cost = 0.5 * (cost_class1 + cost_class0) # take into account both class 1 and class 0\n",
    "    macro_cost = tf.reduce_mean(cost) # average on all labels\n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23008ca1",
   "metadata": {},
   "source": [
    "## Compile and Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "075368ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "# model weights will be saved with this name\n",
    "model_name = 'lstm_small_categories'\n",
    "# in this directory\n",
    "model_path = pathlib.Path('/mnt/tag_data/models/')\n",
    "model_path.mkdir(parent=True)\n",
    "model_savefile = model_path/model_name.with_suffix('.hdf5')\n",
    "# callbacks to automatically monitor/stop training if needed. Set them in the fit call\n",
    "es=EarlyStopping(monitor='val_loss',patience=10)\n",
    "mc=ModelCheckpoint(str(model_savefile),monitor='val_loss',mode='auto',save_best_only=True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', metrics=[exact_count], optimizer=keras.optimizers.Adam(learning_rate=1e-5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "811a439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "137/137 [==============================] - 149s 1s/step - loss: 0.6437 - exact_count: 0.0701 - val_loss: 0.6508 - val_exact_count: 0.1109\n",
      "Epoch 2/50\n",
      "137/137 [==============================] - 158s 1s/step - loss: 0.6183 - exact_count: 0.0829 - val_loss: 0.6266 - val_exact_count: 0.1150\n",
      "Epoch 3/50\n",
      "137/137 [==============================] - 162s 1s/step - loss: 0.6088 - exact_count: 0.0840 - val_loss: 0.6123 - val_exact_count: 0.0164\n",
      "Epoch 4/50\n",
      "137/137 [==============================] - 155s 1s/step - loss: 0.6021 - exact_count: 0.0594 - val_loss: 0.6001 - val_exact_count: 0.0205\n",
      "Epoch 5/50\n",
      "137/137 [==============================] - 142s 1s/step - loss: 0.5987 - exact_count: 0.0491 - val_loss: 0.5926 - val_exact_count: 0.0308\n",
      "Epoch 6/50\n",
      "137/137 [==============================] - 152s 1s/step - loss: 0.5950 - exact_count: 0.0457 - val_loss: 0.5900 - val_exact_count: 0.0267\n",
      "Epoch 7/50\n",
      "137/137 [==============================] - 152s 1s/step - loss: 0.5933 - exact_count: 0.0443 - val_loss: 0.5872 - val_exact_count: 0.0349\n",
      "Epoch 8/50\n",
      "137/137 [==============================] - 147s 1s/step - loss: 0.5930 - exact_count: 0.0445 - val_loss: 0.5856 - val_exact_count: 0.0349\n",
      "Epoch 9/50\n",
      "137/137 [==============================] - 143s 1s/step - loss: 0.5903 - exact_count: 0.0365 - val_loss: 0.5836 - val_exact_count: 0.0349\n",
      "Epoch 10/50\n",
      "137/137 [==============================] - 148s 1s/step - loss: 0.5902 - exact_count: 0.0459 - val_loss: 0.5840 - val_exact_count: 0.0411\n",
      "Epoch 11/50\n",
      "137/137 [==============================] - 143s 1s/step - loss: 0.5878 - exact_count: 0.0496 - val_loss: 0.5821 - val_exact_count: 0.0370\n",
      "Epoch 12/50\n",
      "137/137 [==============================] - 141s 1s/step - loss: 0.5873 - exact_count: 0.0514 - val_loss: 0.5815 - val_exact_count: 0.0431\n",
      "Epoch 13/50\n",
      "137/137 [==============================] - 136s 991ms/step - loss: 0.5850 - exact_count: 0.0505 - val_loss: 0.5803 - val_exact_count: 0.0493\n",
      "Epoch 14/50\n",
      "137/137 [==============================] - 136s 993ms/step - loss: 0.5850 - exact_count: 0.0525 - val_loss: 0.5797 - val_exact_count: 0.0452\n",
      "Epoch 15/50\n",
      "137/137 [==============================] - 136s 995ms/step - loss: 0.5849 - exact_count: 0.0537 - val_loss: 0.5774 - val_exact_count: 0.0575\n",
      "Epoch 16/50\n",
      "137/137 [==============================] - 136s 994ms/step - loss: 0.5833 - exact_count: 0.0514 - val_loss: 0.5783 - val_exact_count: 0.0493\n",
      "Epoch 17/50\n",
      "137/137 [==============================] - 136s 995ms/step - loss: 0.5832 - exact_count: 0.0539 - val_loss: 0.5783 - val_exact_count: 0.0513\n",
      "Epoch 18/50\n",
      "137/137 [==============================] - 136s 991ms/step - loss: 0.5814 - exact_count: 0.0562 - val_loss: 0.5765 - val_exact_count: 0.0575\n",
      "Epoch 19/50\n",
      "137/137 [==============================] - 136s 993ms/step - loss: 0.5822 - exact_count: 0.0573 - val_loss: 0.5761 - val_exact_count: 0.0534\n",
      "Epoch 20/50\n",
      "137/137 [==============================] - 136s 994ms/step - loss: 0.5810 - exact_count: 0.0528 - val_loss: 0.5754 - val_exact_count: 0.0595\n",
      "Epoch 21/50\n",
      "137/137 [==============================] - 136s 996ms/step - loss: 0.5816 - exact_count: 0.0594 - val_loss: 0.5751 - val_exact_count: 0.0595\n",
      "Epoch 22/50\n",
      "137/137 [==============================] - 136s 993ms/step - loss: 0.5789 - exact_count: 0.0562 - val_loss: 0.5754 - val_exact_count: 0.0534\n",
      "Epoch 23/50\n",
      "137/137 [==============================] - 136s 991ms/step - loss: 0.5800 - exact_count: 0.0628 - val_loss: 0.5728 - val_exact_count: 0.0595\n",
      "Epoch 24/50\n",
      "137/137 [==============================] - 134s 975ms/step - loss: 0.5792 - exact_count: 0.0630 - val_loss: 0.5749 - val_exact_count: 0.0534\n",
      "Epoch 25/50\n",
      "137/137 [==============================] - 133s 974ms/step - loss: 0.5802 - exact_count: 0.0575 - val_loss: 0.5714 - val_exact_count: 0.0678\n",
      "Epoch 26/50\n",
      "137/137 [==============================] - 133s 973ms/step - loss: 0.5784 - exact_count: 0.0621 - val_loss: 0.5717 - val_exact_count: 0.0637\n",
      "Epoch 27/50\n",
      "137/137 [==============================] - 134s 976ms/step - loss: 0.5782 - exact_count: 0.0633 - val_loss: 0.5716 - val_exact_count: 0.0616\n",
      "Epoch 28/50\n",
      "137/137 [==============================] - 134s 978ms/step - loss: 0.5776 - exact_count: 0.0619 - val_loss: 0.5710 - val_exact_count: 0.0678\n",
      "Epoch 29/50\n",
      "137/137 [==============================] - 134s 981ms/step - loss: 0.5761 - exact_count: 0.0635 - val_loss: 0.5698 - val_exact_count: 0.0637\n",
      "Epoch 30/50\n",
      "137/137 [==============================] - 135s 984ms/step - loss: 0.5776 - exact_count: 0.0637 - val_loss: 0.5688 - val_exact_count: 0.0760\n",
      "Epoch 31/50\n",
      "137/137 [==============================] - 136s 995ms/step - loss: 0.5769 - exact_count: 0.0569 - val_loss: 0.5691 - val_exact_count: 0.0678\n",
      "Epoch 32/50\n",
      "137/137 [==============================] - 141s 1s/step - loss: 0.5766 - exact_count: 0.0637 - val_loss: 0.5695 - val_exact_count: 0.0698\n",
      "Epoch 33/50\n",
      "137/137 [==============================] - 142s 1s/step - loss: 0.5749 - exact_count: 0.0644 - val_loss: 0.5676 - val_exact_count: 0.0637\n",
      "Epoch 34/50\n",
      "137/137 [==============================] - 141s 1s/step - loss: 0.5752 - exact_count: 0.0662 - val_loss: 0.5673 - val_exact_count: 0.0719\n",
      "Epoch 35/50\n",
      "137/137 [==============================] - 141s 1s/step - loss: 0.5738 - exact_count: 0.0662 - val_loss: 0.5666 - val_exact_count: 0.0739\n",
      "Epoch 36/50\n",
      "137/137 [==============================] - 142s 1s/step - loss: 0.5741 - exact_count: 0.0639 - val_loss: 0.5676 - val_exact_count: 0.0595\n",
      "Epoch 37/50\n",
      "137/137 [==============================] - 140s 1s/step - loss: 0.5728 - exact_count: 0.0621 - val_loss: 0.5659 - val_exact_count: 0.0698\n",
      "Epoch 38/50\n",
      "137/137 [==============================] - 142s 1s/step - loss: 0.5728 - exact_count: 0.0669 - val_loss: 0.5680 - val_exact_count: 0.0657\n",
      "Epoch 39/50\n",
      "137/137 [==============================] - 148s 1s/step - loss: 0.5733 - exact_count: 0.0580 - val_loss: 0.5661 - val_exact_count: 0.0595\n",
      "Epoch 40/50\n",
      "137/137 [==============================] - 148s 1s/step - loss: 0.5714 - exact_count: 0.0669 - val_loss: 0.5661 - val_exact_count: 0.0657\n",
      "Epoch 41/50\n",
      "137/137 [==============================] - 144s 1s/step - loss: 0.5724 - exact_count: 0.0621 - val_loss: 0.5673 - val_exact_count: 0.0595\n",
      "Epoch 42/50\n",
      "137/137 [==============================] - 148s 1s/step - loss: 0.5713 - exact_count: 0.0690 - val_loss: 0.5667 - val_exact_count: 0.0616\n",
      "Epoch 43/50\n",
      "137/137 [==============================] - 145s 1s/step - loss: 0.5716 - exact_count: 0.0644 - val_loss: 0.5648 - val_exact_count: 0.0575\n",
      "Epoch 44/50\n",
      "137/137 [==============================] - 147s 1s/step - loss: 0.5707 - exact_count: 0.0614 - val_loss: 0.5639 - val_exact_count: 0.0678\n",
      "Epoch 45/50\n",
      "137/137 [==============================] - 152s 1s/step - loss: 0.5705 - exact_count: 0.0717 - val_loss: 0.5624 - val_exact_count: 0.0678\n",
      "Epoch 46/50\n",
      "137/137 [==============================] - 144s 1s/step - loss: 0.5684 - exact_count: 0.0651 - val_loss: 0.5630 - val_exact_count: 0.0616\n",
      "Epoch 47/50\n",
      "137/137 [==============================] - 142s 1s/step - loss: 0.5686 - exact_count: 0.0662 - val_loss: 0.5623 - val_exact_count: 0.0637\n",
      "Epoch 48/50\n",
      "137/137 [==============================] - 141s 1s/step - loss: 0.5683 - exact_count: 0.0708 - val_loss: 0.5622 - val_exact_count: 0.0678\n",
      "Epoch 49/50\n",
      "137/137 [==============================] - 141s 1s/step - loss: 0.5693 - exact_count: 0.0703 - val_loss: 0.5630 - val_exact_count: 0.0698\n",
      "Epoch 50/50\n",
      "137/137 [==============================] - 140s 1s/step - loss: 0.5687 - exact_count: 0.0690 - val_loss: 0.5597 - val_exact_count: 0.0719\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f8b14",
   "metadata": {},
   "source": [
    "## Testing\n",
    "Simple validation metrics are available in the training history\n",
    "But it can be useful to play around with the fit model and test or other data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1678af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8f462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to class labels (one-hot encoding)\n",
    "y_pred[y_pred>=0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the exact match ratio\n",
    "exact_count(y_test, y_pred).numpy().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ca457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=final_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618dcec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b08dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d08e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
